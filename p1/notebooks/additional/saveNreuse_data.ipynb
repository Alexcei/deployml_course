{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно, мы сохраняем обученную модель для дальнейшего использования.\n",
    "\n",
    "Хорошо сохранять не только модель, но и некоторые методы, которые используются с ней (скалирование и т.п.)\n",
    "\n",
    "Результаты методов normalization или standardization требуют тех же параметров от входящих данных (мин, макс, стд, сред), вы должны проверять это тестами или сохранять вместе с моделью.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0, значения обуч.: min=-11.856, max=0.526, test: min=-11.270, max=0.085\n",
      ">1, значения обуч.: min=-6.388, max=6.507, test: min=-5.581, max=5.926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# рандомный дата сет и его разделение на обуч/тест\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# сравним обуч / тест\n",
    "for i in range(X_test.shape[1]):\n",
    "    print('>%d, значения обуч.: min=%.3f, max=%.3f, test: min=%.3f, max=%.3f' %\n",
    "         (i, X_train[:, i].min(), X_train[:, i].max(),\n",
    "             X_test[:, i].min(), X_test[:, i].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0, значения обуч.: min=0.000, max=1.000, test: min=0.047, max=0.964\n",
      ">1, значения обуч.: min=0.000, max=1.000, test: min=0.063, max=0.955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# повторяем\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# МинМах - скалирование каждого значения от 0 до 1\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# применяем к наборам данных\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "for i in range(X_test.shape[1]):\n",
    "    print('>%d, значения обуч.: min=%.3f, max=%.3f, test: min=%.3f, max=%.3f' %\n",
    "         (i, X_train_scaled[:, i].min(), X_train_scaled[:, i].max(),\n",
    "             X_test_scaled[:, i].min(), X_test_scaled[:, i].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применяем к модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pickle import dump\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# определяем модель \n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# сохраняем модель\n",
    "dump(model, open('model.pkl', 'wb'))\n",
    "# сохраняем скалирование \n",
    "dump(scaler, open('scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверяем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тест на необработанных данных\n",
      ">0, min=-11.270, max=0.085\n",
      ">1, min=-5.581, max=5.926\n",
      "Test Accuracy: 0.9696969696969697\n",
      "\n",
      "\n",
      "Тест на скалируемых данных\n",
      ">0, min=0.047, max=0.964\n",
      ">1, min=0.063, max=0.955\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pickle import load\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "_, X_test, _, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# загружаем модель\n",
    "model = load(open('model.pkl', 'rb'))\n",
    "scaler = load(open('scaler.pkl', 'rb'))\n",
    "\n",
    "print('Тест на необработанных данных')\n",
    "for i in range(X_test.shape[1]):\n",
    "    print('>%d, min=%.3f, max=%.3f' % (i, X_test[:, i].min(), X_test[:, i].max()))\n",
    "\n",
    "# без скалирования\n",
    "yhat = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Test Accuracy:', acc)    \n",
    "\n",
    "print('\\n')\n",
    "# скалируем данные \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print('Тест на скалируемых данных')\n",
    "for i in range(X_test_scaled.shape[1]):\n",
    "    print('>%d, min=%.3f, max=%.3f' % (i, X_test_scaled[:, i].min(), X_test_scaled[:, i].max()))\n",
    "\n",
    "yhat = model.predict(X_test_scaled)\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Test Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 DS",
   "language": "python",
   "name": "python3_ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
