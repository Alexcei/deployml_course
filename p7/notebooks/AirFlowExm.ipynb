{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаг 1\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.models import Variable\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# + создаем функции\n",
    "def my_func():\n",
    "    return \"Dream func - Всё необходимое в одной функции\"\n",
    "\n",
    "\n",
    "# установка Apache Airflow Variables \n",
    "# /admin/variable/ -> Create new | или загрузить json с переменными\n",
    "AUTHOR = Variable.get(\"desc_dict\", deserialize_json=True)['dml']['author']\n",
    "\n",
    "\n",
    "\n",
    "# Шаг 2 (аргументы)\n",
    "default_args = {\n",
    "    \"owner\": \"airflow\",\n",
    "    \"depends_on_past\": False,              # зависимость от прошлого результата\n",
    "    \"start_date\": datetime(2020, 12, 21),  # первая дата выполнения -> airflow.utils.dates.days_ago(#)\n",
    "    # \"end_date\":                          # последняя дата (выполнять до)\n",
    "    \"retries\": 1,                          # повторных запусков\n",
    "    \"retry_delay\": timedelta(minutes=2),   # повторный запуск через кол-во минут\n",
    "    # email | email_on_failure | email_on_retry \n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'wait_for_downstream': False,\n",
    "    # 'dag': dag,\n",
    "    # 'sla': timedelta(hours=2),\n",
    "    # 'execution_timeout': timedelta(seconds=300),\n",
    "    # 'on_failure_callback': some_function,\n",
    "    # 'on_success_callback': some_other_function,\n",
    "    # 'on_retry_callback': another_function,\n",
    "    # 'sla_miss_callback': yet_another_function,\n",
    "    # 'trigger_rule': 'all_success'\n",
    "}\n",
    "\n",
    "\n",
    "# Шаг 3 (описание)\n",
    "dag = DAG(\n",
    "    \"steps_in_DAG\",                          # имя процесса\n",
    "    description=\"Все шаги для создания DAG\", # описание\n",
    "    schedule_interval=\"0 0 * * *\",           # аналогично, как в cron\n",
    "    default_args=default_args,\n",
    "    catchup=False                            # catchup - концепция\n",
    "    \n",
    "    # catchup - DAG разбивается на шаги, каждый шаг - это отдельный запуск. \n",
    "    # При параметре True, DAG будет выполнятся по отдельности, без очередности (каждый шаг в разный момент)\n",
    ")\n",
    "\n",
    "\n",
    "# Шаг 4\n",
    "task1 = DummyOperator(task_id=\"dummy_task\",\n",
    "                      retries=3,\n",
    "                      dag=dag)\n",
    "\n",
    "\n",
    "# Документирование каждого задани\n",
    "task1.doc_md = \"\"\"\n",
    "# Task 1\n",
    "\n",
    "Здес описано задание 1 для Apache Airflow\n",
    "\"\"\"\n",
    "\n",
    "dag.doc_md = __doc_\n",
    "\n",
    "\n",
    "task2 = PythonOperator(task_id=\"my_func_task\",\n",
    "                       python_callable=my_func,\n",
    "                       dag=dag)\n",
    "\n",
    "\n",
    "# можно добавить визуальный шаблон\n",
    "templated_command = \"\"\"\n",
    "echo \"{{ var.value.AUTHOR }}\"\n",
    "echo \"{{ params.best_param }}\"\n",
    "\"\"\"\n",
    "\n",
    "task3 = BashOperator(\n",
    "    task_id='templated',\n",
    "    depends_on_past=False,\n",
    "    bash_command=templated_command,\n",
    "    params={'best_param': 'О, да! Ты подобрал лучшие параметры'},\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Шаг 5 (установка последовательности)\n",
    "# установка последовательности\n",
    "task1 >> task2 >> task3\n",
    "\n",
    "# равнозначно\n",
    "# task2.set_upstream(task1)\n",
    "\n",
    "# task1.set_downstream([task2, task3])\n",
    "# task1 >> [task2, task3]\n",
    "\n",
    "# в обратном порядке\n",
    "# [task2, task3] << task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sch_int.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AirFlow операторы\n",
    "\n",
    "- **Sensors (на [git](https://github.com/apache/airflow/tree/master/airflow/sensors)):**\n",
    "    * HdsfSensor - ожидает появления нового файла в таблице Hadoop\n",
    "    * NamedHivePartitionSensor - проверяет доступность партиций в Hive таблице\n",
    "    * DateTime / TimeDelta- зависимость от времени\n",
    "    * Filesystem / FileSensor - если появился новый файл в заданной директории\n",
    "    * Python - если какой-нибудь(указаный в условиях) python файл вернул True\n",
    "\n",
    "**!NB** Вы всегда можете добавить свой собственный сенсор:\n",
    "```python\n",
    "from airflow.sensors.base import BaseSensorOperator\n",
    "\n",
    "class MySensor(BaseSensorOperator):\n",
    "    \n",
    "    @apply_defaults\n",
    "    def __init__(self, *, параметры для данного сенсор, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ...\n",
    "\n",
    "    def poke(self, context):\n",
    "        ...\n",
    "    \n",
    "```\n",
    "\n",
    "---------------------------------------------------------------------------------------\n",
    "\n",
    " \n",
    "- **Operators (на [git](https://github.com/apache/airflow/tree/master/airflow/operators)):**\n",
    "    * BashOperator\n",
    "    * PythonOperator\n",
    "    * HiveOperator\n",
    "    * HttpOperator\n",
    "    * Postgres/MySqlOperator\n",
    "    * SubDag\n",
    "    \n",
    "**!NB** Вы всегда можете добавить свой собственный оператор:\n",
    "```python\n",
    "from airflow.models import BaseOperator\n",
    "\n",
    "class MyBestOperator(BaseOperator):\n",
    "    \n",
    "    @apply_defaults\n",
    "    def __init__(self, *, параметры для данного оператора, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ...\n",
    "        \n",
    "    def execute(self, context):\n",
    "        ...\n",
    "```\n",
    "\n",
    "---------------------------------------------------------------------------------------\n",
    "    \n",
    "- **Transfers:**\n",
    "    * MySql/PostgresToHiveTransfes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AirFlow + Kedro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow import DAG\n",
    "from kedro_airflow.runner import AirflowRunner\n",
    "from kedro.framework.context import load_context\n",
    "\n",
    "\n",
    "# Установка аргументов\n",
    "default_args = {\n",
    "    \"owner\": \"kedro\",\n",
    "    \"start_date\": datetime(2020, 12, 21),\n",
    "    \"depends_on_past\": False,\n",
    "    \"wait_for_downstream\": True,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=10),\n",
    "}\n",
    "\n",
    "\n",
    "# Функция создания контекста и определения pipeline для запуска\n",
    "def process_context(data_catalog, **airflow_context):\n",
    "    \n",
    "    for key in [\"dag\", \"conf\", \"macros\", \"task\", \"task_instance\", \"ti\", \"var\"]:\n",
    "        del airflow_context[key]\n",
    "        \n",
    "    data_catalog.add_feed_dict({\"airflow_context\": airflow_context}, replace=True)\n",
    "\n",
    "    parameters = data_catalog.load(\"parameters\")\n",
    "    parameters[\"airflow_de_pipeline\"] = airflow_context[\"de\"]\n",
    "    data_catalog.save(\"parameters\", parameters)\n",
    "\n",
    "    return data_catalog\n",
    "\n",
    "\n",
    "# Создаем DAG для Kedro\n",
    "dag = DAG(\n",
    "    \"bike\",\n",
    "    description=\"DE pipeline для bike модели\",\n",
    "    default_args=default_args,\n",
    "    schedule_interval=None,\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "\n",
    "# загружаем контекст kedro (как обычно, как в Bonobo)\n",
    "_context = load_context(\"\")\n",
    "data_catalog = _context.catalog\n",
    "pipeline = _context.pipeline\n",
    "\n",
    "# создаем airflow процесс\n",
    "runner = AirflowRunner(\n",
    "                       dag=dag,\n",
    "                       process_context=process_context,\n",
    "                       operator_arguments=dict(),\n",
    "                       )\n",
    "\n",
    "# инит процесса == task1 >> task2 >> ...\n",
    "runner.run(pipeline, data_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kedro_airflow может больше, kedro сам может создать себе готовый код для airflow\n",
    "# kedro airflow create"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
